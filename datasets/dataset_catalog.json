{
  "catalog_metadata": {
    "project": "Do You Mean...?: Fixing User Intent Without Annoying Them",
    "hypothesis": "Autocomplete and intent detection systems often incorrectly correct users in ways that alter their original intent. Evaluating whether model rewrites preserve user intent is necessary.",
    "created": "2026-02-08",
    "total_datasets": 14
  },
  "datasets": [
    {
      "id": 1,
      "name": "GitHub Typo Corpus",
      "category": "Query Correction / Spell Correction",
      "source_url": "https://github.com/mhagiwara/github-typo-corpus",
      "huggingface_url": null,
      "paper_url": "https://aclanthology.org/2020.lrec-1.835/",
      "size": "350,000+ edits across 65M characters in 15+ languages",
      "format": "JSONL (one commit object per line with source text, target text, file path, language, typo probability score)",
      "description": "A large-scale multilingual dataset of misspellings and grammatical errors harvested from GitHub commit messages. Each entry contains the original misspelled text and its correction, along with metadata about the edit.",
      "relevance": "Directly relevant as a source of real-world misspelling-correction pairs. Can be used to evaluate whether spelling corrections preserve the original developer intent, especially in technical contexts where jargon may be incorrectly 'corrected'.",
      "download_command": "git clone https://github.com/mhagiwara/github-typo-corpus.git",
      "license": "CC-BY-4.0",
      "priority": "HIGH"
    },
    {
      "id": 2,
      "name": "NeuSpell Benchmark (BEA-60K)",
      "category": "Query Correction / Spell Correction",
      "source_url": "https://github.com/neuspell/neuspell",
      "huggingface_url": null,
      "paper_url": "https://aclanthology.org/2020.emnlp-demos.21/",
      "size": "Training: 1.6M sentences (from One Billion Word benchmark); Evaluation: BEA-60K with 60,000 real-world misspelling pairs",
      "format": "Text files (clean/noised sentence pairs: bea60k.txt and bea60k.noise.txt)",
      "description": "A neural spelling correction toolkit with curated benchmark datasets. The BEA-60K evaluation set contains naturally occurring real-world misspellings. Training data is synthetically generated using four noising strategies (random, word-level, probabilistic, and combined).",
      "relevance": "The BEA-60K set contains real-world misspellings ideal for evaluating whether correction systems alter user intent. The multiple noising strategies can be used to generate controlled test cases for intent preservation studies.",
      "download_command": "pip install neuspell && cd data/traintest && python download_datafiles.py",
      "license": "MIT",
      "priority": "HIGH"
    },
    {
      "id": 3,
      "name": "Birkbeck Spelling Error Corpus",
      "category": "Query Correction / Spell Correction",
      "source_url": "https://www.dcs.bbk.ac.uk/~ROGER/corpora.html",
      "huggingface_url": null,
      "paper_url": null,
      "size": "36,133 misspellings of 6,136 target words",
      "format": "Text files (misspelling-correction pairs organized by source)",
      "description": "A collection of English misspelling corpora gathered from spelling tests and free writing by schoolchildren, university students, and adult literacy students. Contains native and non-native speaker errors from diverse populations.",
      "relevance": "Provides naturalistic misspelling data where corrections may or may not preserve the user's intended word. Useful for studying cases where autocorrect might 'correct' to the wrong target word, changing user intent.",
      "download_command": "Download from https://www.dcs.bbk.ac.uk/~ROGER/corpora.html or Oxford Text Archive (627 KB)",
      "license": "Research use",
      "priority": "MEDIUM"
    },
    {
      "id": 4,
      "name": "ClariQ",
      "category": "Clarification Question Generation",
      "source_url": "https://github.com/aliannejadi/ClariQ",
      "huggingface_url": null,
      "paper_url": "https://arxiv.org/abs/2009.11352",
      "size": "18,000 single-turn conversations; 1.8M synthetic multi-turn conversations",
      "format": "TSV files (topics/queries, facets, clarifying questions, user answers, clarification need labels, ambiguity levels 1-4)",
      "description": "An extended version of Qulac created for the SCAI workshop challenge on conversational search clarification. Contains queries with ambiguity annotations (1-4 scale), clarifying questions, and user answers. Includes both single-turn and multi-turn conversation variants.",
      "relevance": "Directly relevant for studying when systems should ask clarifying questions instead of making autocorrections. The ambiguity level annotations (1-4) can help train models to recognize when a query is ambiguous enough that correction would risk altering intent.",
      "download_command": "git clone https://github.com/aliannejadi/ClariQ.git",
      "license": "Research use",
      "priority": "HIGH"
    },
    {
      "id": 5,
      "name": "Qulac",
      "category": "Clarification Question Generation",
      "source_url": "https://github.com/aliannejadi/qulac",
      "huggingface_url": null,
      "paper_url": "https://arxiv.org/abs/1907.06554",
      "size": "10,000+ question-answer pairs across 198 TREC topics with 762 facets",
      "format": "JSON (topic_facet_id, topic_facet_question_id, clarifying questions, answers, facets)",
      "description": "A dataset for asking Questions for Lack of Clarity in open-domain information-seeking conversations. Built on TREC Web Track 2009-2012. Each entry contains a topic (query), facets (possible user intents), clarifying questions, and crowdsourced answers.",
      "relevance": "The facet structure explicitly models multiple possible intents behind a single query, which is exactly the scenario where autocorrect can go wrong. Can be used to study when a system should clarify rather than correct.",
      "download_command": "git clone https://github.com/aliannejadi/qulac.git",
      "license": "Research use",
      "priority": "HIGH"
    },
    {
      "id": 6,
      "name": "ClarQ",
      "category": "Clarification Question Generation",
      "source_url": "https://github.com/vaibhav4595/ClarQ",
      "huggingface_url": null,
      "paper_url": "https://aclanthology.org/2020.acl-main.651/",
      "size": "~2 million examples across 173 StackExchange domains",
      "format": "Post-comment tuples (post with ambiguous content, clarification question, answer)",
      "description": "A large-scale, diverse dataset of clarification questions generated using a bootstrapping framework based on self-supervision, extracted from StackExchange post-comment tuples across 173 domains.",
      "relevance": "Massive scale dataset of real-world ambiguous posts paired with clarifying questions. Useful for training models to detect when user input is ambiguous and a clarification question would be more appropriate than an autocorrection.",
      "download_command": "git clone https://github.com/vaibhav4595/ClarQ.git (data available via Google Drive link in repo)",
      "license": "Research use",
      "priority": "MEDIUM"
    },
    {
      "id": 7,
      "name": "BANKING77",
      "category": "Intent Detection Benchmark",
      "source_url": "https://huggingface.co/datasets/PolyAI/banking77",
      "huggingface_url": "https://huggingface.co/datasets/PolyAI/banking77",
      "paper_url": "https://arxiv.org/abs/2003.04807",
      "size": "13,083 examples (train: 10,003; test: 3,080) across 77 intents",
      "format": "HuggingFace Dataset (text, label columns)",
      "description": "A fine-grained intent detection dataset in the banking domain. Contains customer service queries labeled with one of 77 banking-related intents such as card payment issues, transfers, and account management.",
      "relevance": "Fine-grained intent labels allow measuring whether autocorrections change the predicted intent class. The narrow banking domain means small text changes can flip between closely related intents (e.g., 'card_payment_fee_charged' vs 'card_payment_wrong_exchange_rate').",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('PolyAI/banking77')",
      "license": "CC-BY-4.0",
      "priority": "HIGH"
    },
    {
      "id": 8,
      "name": "CLINC150 (clinc_oos)",
      "category": "Intent Detection Benchmark",
      "source_url": "https://huggingface.co/datasets/DeepPavlov/clinc150",
      "huggingface_url": "https://huggingface.co/datasets/DeepPavlov/clinc150",
      "paper_url": "https://arxiv.org/abs/1909.02027",
      "size": "23,700 examples (train: 15,200; val: 3,100; test: 5,500) across 150 intents + out-of-scope",
      "format": "HuggingFace Dataset (utterance, label columns)",
      "description": "A multi-domain intent classification dataset with 150 in-scope intent classes across 10 domains plus an out-of-scope class. Domains include banking, travel, work, and more.",
      "relevance": "The 150 fine-grained intents across 10 domains plus the out-of-scope class make this ideal for measuring intent preservation. An autocorrection that shifts a query from in-scope to out-of-scope (or between intents) demonstrates intent violation.",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('DeepPavlov/clinc150')",
      "license": "CC-BY-3.0",
      "priority": "HIGH"
    },
    {
      "id": 9,
      "name": "SNIPS NLU Benchmark",
      "category": "Intent Detection Benchmark",
      "source_url": "https://huggingface.co/datasets/DeepPavlov/snips",
      "huggingface_url": "https://huggingface.co/datasets/DeepPavlov/snips",
      "paper_url": "https://arxiv.org/abs/1805.10190",
      "size": "~14,484 examples across 7 intents with slot annotations",
      "format": "HuggingFace Dataset (utterance, intent label, slot annotations)",
      "description": "A voice assistant intent detection dataset with 7 intents: SearchCreativeWork, GetWeather, BookRestaurant, PlayMusic, AddToPlaylist, RateBook, SearchScreeningEvent. Includes slot/entity annotations.",
      "relevance": "Voice-oriented dataset where slot values are critical to intent. Autocorrect errors in slot values (restaurant names, song titles, locations) would directly change the user's intended action. The slot annotations allow measuring entity-level intent preservation.",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('DeepPavlov/snips')",
      "license": "CC0-1.0",
      "priority": "MEDIUM"
    },
    {
      "id": 10,
      "name": "PAWS (Paraphrase Adversaries from Word Scrambling)",
      "category": "Paraphrase / Semantic Similarity",
      "source_url": "https://huggingface.co/datasets/google-research-datasets/paws",
      "huggingface_url": "https://huggingface.co/datasets/google-research-datasets/paws",
      "paper_url": "https://arxiv.org/abs/1904.01130",
      "size": "108,463 human-labeled pairs + 656K noisily labeled pairs (labeled_final: train 49,401; dev 8,000; test 8,000)",
      "format": "HuggingFace Dataset (sentence1, sentence2, label: 0=not_paraphrase/1=paraphrase)",
      "description": "Sentence pairs generated via word swapping and back translation that have high lexical overlap but may or may not be paraphrases. The adversarial construction means word-order changes can completely change meaning despite similar vocabulary.",
      "relevance": "Directly tests the core hypothesis: small word-level changes (like those made by autocorrect) can alter meaning. PAWS pairs demonstrate that high lexical overlap does NOT guarantee semantic equivalence, which is exactly the failure mode of naive autocorrect systems.",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('google-research-datasets/paws', 'labeled_final')",
      "license": "Other (free to use with attribution to Google)",
      "priority": "HIGH"
    },
    {
      "id": 11,
      "name": "MRPC (Microsoft Research Paraphrase Corpus)",
      "category": "Paraphrase / Semantic Similarity",
      "source_url": "https://huggingface.co/datasets/glue/viewer/mrpc",
      "huggingface_url": "https://huggingface.co/datasets/glue",
      "paper_url": "https://aclanthology.org/I05-5002/",
      "size": "5,801 sentence pairs (train: ~3,668; test: ~1,725)",
      "format": "HuggingFace Dataset via GLUE (sentence1, sentence2, label: 0=not_equivalent/1=equivalent)",
      "description": "Sentence pairs extracted from online news sources with human annotations indicating whether the sentences are semantically equivalent. Part of the GLUE benchmark.",
      "relevance": "Provides a standard benchmark for training and evaluating semantic equivalence classifiers. Can be used to build models that assess whether an autocorrected sentence preserves the meaning of the original.",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('glue', 'mrpc')",
      "license": "Other (Microsoft Research)",
      "priority": "MEDIUM"
    },
    {
      "id": 12,
      "name": "STS-B (Semantic Textual Similarity Benchmark)",
      "category": "Paraphrase / Semantic Similarity",
      "source_url": "https://huggingface.co/datasets/sentence-transformers/stsb",
      "huggingface_url": "https://huggingface.co/datasets/sentence-transformers/stsb",
      "paper_url": "https://arxiv.org/abs/1708.00055",
      "size": "8,628 sentence pairs (train: 5,749; dev: 1,500; test: 1,379)",
      "format": "HuggingFace Dataset (sentence1, sentence2, similarity_score: continuous 0.0-5.0 normalized to 0-1)",
      "description": "Sentence pairs drawn from news headlines, video captions, image captions, and natural language inference data. Each pair is annotated with a continuous similarity score from 0 to 5, providing fine-grained semantic similarity assessment.",
      "relevance": "The continuous similarity scores (rather than binary labels) allow measuring degrees of intent preservation. An autocorrection that reduces similarity from 4.5 to 3.0 represents a measurable degradation in intent preservation.",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('sentence-transformers/stsb')",
      "license": "Other (Creative Commons Attribution-ShareAlike 4.0 via GLUE)",
      "priority": "HIGH"
    },
    {
      "id": 13,
      "name": "AmazonQAC (Amazon Query Autocomplete)",
      "category": "Query Reformulation / Autocomplete",
      "source_url": "https://huggingface.co/datasets/amazon/AmazonQAC",
      "huggingface_url": "https://huggingface.co/datasets/amazon/AmazonQAC",
      "paper_url": "https://arxiv.org/abs/2411.04129",
      "size": "395 million training samples; 20,000 test samples",
      "format": "Parquet (query_id, session_id, past_search_terms, prefix, prefix_typed_time, final_search_term, search_time)",
      "description": "A large-scale naturalistic query autocomplete dataset from Amazon Search logs. Contains real user-typed prefix sequences leading to final search terms, with session metadata and timestamps. PII has been removed and queries are filtered to those appearing in 4+ sessions.",
      "relevance": "The most directly relevant dataset for studying autocomplete intent preservation. Contains actual prefix-to-completion pairs from real users, allowing measurement of whether autocomplete suggestions match what users actually intended to search for.",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('amazon/AmazonQAC')",
      "license": "Apache-2.0",
      "priority": "HIGH"
    },
    {
      "id": 14,
      "name": "ASSET (Abstractive Sentence Simplification Evaluation and Tuning)",
      "category": "Text Rewriting",
      "source_url": "https://huggingface.co/datasets/asset",
      "huggingface_url": "https://huggingface.co/datasets/asset",
      "paper_url": "https://aclanthology.org/2020.acl-main.424/",
      "size": "2,359 original sentences, each with 10 crowdsourced simplifications (23,590 pairs total for evaluation)",
      "format": "HuggingFace Dataset (original sentence, multiple reference simplifications)",
      "description": "A multi-reference dataset for evaluating text simplification. Each source sentence has 10 crowdsourced simplifications that encompass multiple rewriting transformations (splitting, deletion, paraphrasing), unlike datasets with only single-transformation rewrites.",
      "relevance": "Text simplification is a form of rewriting that must preserve core meaning. The multiple references per sentence allow studying which rewriting strategies preserve intent and which inadvertently alter it. Provides a template for evaluating autocorrect rewrites.",
      "download_command": "from datasets import load_dataset\ndataset = load_dataset('asset', 'ratings')",
      "license": "CC-BY-SA-4.0",
      "priority": "MEDIUM"
    }
  ]
}
